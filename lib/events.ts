/*
 * Copyright Â© 2021 Atomist, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import {
	datalog,
	EventHandler,
	repository,
	status,
	subscription,
} from "@atomist/skill";
import * as fs from "fs-extra";
import * as _ from "lodash";
import * as path from "path";

import { Configuration } from "./configuration";
import { readLayers } from "./layers";
import { TrivyReport } from "./report";

export const onDockerImage: EventHandler<
	subscription.datalog.OnDockerImage,
	Configuration
> = async ctx => {
	const image = ctx.data.image;
	const digest = ctx.data.image.digest;
	const commit = ctx.data.commit;
	const registry = ctx.data.registry;

	const credential = {
		token: ctx.data.commit.repo.org.installationToken,
		permissions: {},
	};

	const project = await ctx.project.load(
		repository.gitHub({
			owner: commit.repo.org.name,
			repo: commit.repo.name,
			credential,
			sha: commit.sha,
		}),
		process.env.ATOMIST_HOME,
	);

	const env = { ...process.env };
	if (registry) {
		switch (registry.type) {
			case subscription.datalog.DockerRegistryType.Gcr:
				env.TRIVY_USERNAME = "";
				env.GOOGLE_APPLICATION_CREDENTIALS = project.path(
					"google_creds.json",
				);
				await fs.writeFile(
					project.path("google_creds.json"),
					registry.secret,
				);
				break;
			default:
				env.TRIVY_USERNAME = registry.username;
				env.TRIVY_PASSWORD = registry.secret;
				break;
		}
	}

	const outputFile = path.join(project.path("output.json"));

	const cmd = "trivy";
	const args = [
		"--cache-dir",
		path.join(process.env.ATOMIST_HOME, ".cache"),
		"image",
		"--exit-code",
		"1",
	];

	args.push("--format", "json");
	args.push("--output", outputFile, "--no-progress");

	const imageName =
		registry.type !== subscription.datalog.DockerRegistryType.DockerHub
			? `${image.repository.host}/${image.repository.name}`
			: image.repository.name;
	args.push(`${imageName}@${image.digest}`);

	const result = await project.spawn(cmd, args, { env });

	if (result.status === 0) {
		return status.success(`No vulnerabilities found on \`${imageName}\``);
	} else {
		const report: TrivyReport = JSON.parse(
			(await fs.readFile(outputFile)).toString(),
		);

		const vuls = _.values(
			_.groupBy(
				_.flattenDeep(
					report
						.filter(r => r.Vulnerabilities)
						.map(r => r.Vulnerabilities),
				),
				"VulnerabilityID",
			),
		);

		await ctx.audit.log(
			`Vulnerabilities ${commit.sha}: ${_.flattenDeep(vuls || [])
				.map(v => `${v.VulnerabilityID} - ${v.Severity}`)
				.join(", ")}`,
		);

		const vulsChunks = _.chunk(vuls, 25);

		for (const vulChunk of vulsChunks) {
			const entities = [
				..._.map(
					_.groupBy(_.flatten(vulChunk), "Layer.Digest"),
					(v, k) =>
						datalog.entity("docker.image/blob", {
							digest: k,
							vulnerabilities: {
								add: v.map(
									vul =>
										`$vulnerability-${vul.VulnerabilityID}`,
								),
							},
						}),
				),
				..._.flatten(
					vulChunk.map(vu =>
						vu
							.filter(v => v.InstalledVersion)
							.map(v =>
								datalog.entity(
									"docker.analysis/package",
									`$package-${v.VulnerabilityID}-${v.PkgName}`,
									{
										name: v.PkgName,
										version: v.InstalledVersion,
									},
								),
							),
					),
				),
				..._.flatten(
					vulChunk.map(vu =>
						vu
							.filter(v => v.FixedVersion)
							.map(v =>
								datalog.entity(
									"docker.analysis/package",
									`$package-fix-${v.VulnerabilityID}-${v.PkgName}`,
									{
										name: v.PkgName,
										version: v.FixedVersion,
									},
								),
							),
					),
				),
				...vulChunk.map(vu => {
					const v = vu[0];

					const affected = vu
						.filter(vup => vup.InstalledVersion)
						.map(
							vup =>
								`$package-${v.VulnerabilityID}-${vup.PkgName}`,
						);
					const fixed = vu
						.filter(vup => vup.FixedVersion)
						.map(
							vup =>
								`$package-fix-${v.VulnerabilityID}-${vup.PkgName}`,
						);

					const cve = datalog.entity(
						"vulnerability/cve",
						`$vulnerability-${v.VulnerabilityID}`,
						{
							"sourceId": v.VulnerabilityID,
							"severity": `:vulnerability.cve.severity/${mapSeverity(
								v.Severity,
							)}`,
							"effectiveSeverity": `:vulnerability.cve.severity/${mapSeverity(
								v.Severity,
							)}`,
							"fixAvailable": v.FixedVersion !== undefined,
							"description": v.Description,
							"title": v.Title,
							"cvssScore": v.CVSS?.nvd?.V2Score?.toString(),
							"created": v.PublishedDate
								? new Date(v.PublishedDate)
								: undefined,
							"lastUpdated": v.LastModifiedDate
								? new Date(v.LastModifiedDate)
								: undefined,
							"docker.analysis/affected":
								affected.length > 0 ? affected : undefined,
							"docker.analysis/fixed":
								fixed.length > 0 ? fixed : undefined,
						},
					);
					return cve;
				}),
				datalog.entity("docker/image", "$image", {
					"digest": digest,
					"docker.analysis/vulnerabilities": {
						add: vulChunk.map(
							v => `$vulnerability-${v[0].VulnerabilityID}`,
						),
					},
					"docker.analysis/affected": {
						add: _.flatten(
							vulChunk.map(v =>
								v
									.filter(v => v.InstalledVersion)
									.map(
										vup =>
											`$package-${vup.VulnerabilityID}-${vup.PkgName}`,
									),
							),
						),
					},
					"docker.analysis/fixed": {
						add: _.flatten(
							vulChunk.map(v =>
								v
									.filter(v => v.FixedVersion)
									.map(
										vup =>
											`$package-fix-${vup.VulnerabilityID}-${vup.PkgName}`,
									),
							),
						),
					},
				}),
			];
			await ctx.datalog.transact(entities);
		}

		const entities = [
			datalog.entity("docker/image", "$image", {
				digest,
			}),
			datalog.entity("analysis/discovery", {
				image: "$image",
				status: ":analysis.discovery.status/FINISHED_SUCCESS",
				source: ":analysis.discovery.source/TRIVY",
			}),
		];

		await ctx.datalog.transact(entities);

		return status.failure(
			`${vuls.length} ${
				vuls.length === 1 ? "vulnerability" : "vulnerabilities"
			} found on \`${imageName}\``,
		);
	}
};

export const onDockerFile: EventHandler<
	subscription.datalog.OnDockerFile,
	Configuration
> = async ctx => {
	const image = ctx.data.image;
	const file = ctx.data.file;

	const imageName =
		ctx.data.registry.type !==
		subscription.datalog.DockerRegistryType.DockerHub
			? `${image.repository.host}/${image.repository.name}`
			: image.repository.name;

	let layerResult;
	try {
		layerResult = await readLayers(ctx);
	} catch (e) {
		return status.success(
			`Failed to map Dockerfile to image layers for \`${imageName}\``,
		);
	}
	const parser = await import("docker-file-parser");
	let lines = _.orderBy([...file.lines], "number");

	const layers = layerResult.map(h => {
		let run = h.command;
		if (h.command.startsWith("/bin/sh -c ")) {
			run = `RUN ${h.command.slice("/bin/sh -c ".length)}`;
			h.command = run;
		}
		const parsed = parser.parse(run)[0];
		for (const line of lines) {
			if (line.instruction === parsed.name) {
				let match = false;
				// RUN
				if (line.argsString === parsed.args) {
					match = true;
				} else if (_.isEqual(line.argsArray, parsed.args)) {
					match = true;
				} else if (_.isEqual(line.argsMap, parsed.args)) {
					match = true;
				}
				if (match) {
					(h as any).line = line.number;
					lines = lines.filter(l => l !== line);
				}
			}
		}
		return h;
	});
	const baselayers = layers.filter(l => (l as any).line === undefined);

	const baseImageFrom = _.orderBy(file.lines, "number")
		.filter(p => p.instruction === "FROM")
		.slice(-1)[0];

	const result: Array<{
		digest: string;
		lineNumber: number;
		command: string;
	}> = _.orderBy(
		[
			...layers
				.filter((l: any) => l.line)
				.map((l: any) => ({
					digest: l.digest,
					lineNumber: l.line,
					command: l.command,
				})),
			...baselayers.map((l: any) => ({
				digest: l.digest,
				lineNumber: baseImageFrom.number,
				command: baseImageFrom.instruction,
			})),
		],
		"lineNumber",
	);

	const entities = [
		// docker.file
		datalog.entity("docker/file", "$file", {
			sha: file.sha,
			path: file.path,
		}),
		// docker.file/lines for each line we are able to map
		..._.uniqBy(result, "lineNumber").map(r =>
			datalog.entity("docker.file/line", `$line-${r.lineNumber}`, {
				file: "$file",
				number: r.lineNumber,
			}),
		),
		// docker.image/blob for each line we are able to map
		...result.map(r =>
			datalog.entity("docker.image/blob", {
				digest: r.digest,
				createdBy: `$line-${r.lineNumber}`,
			}),
		),
	];

	await ctx.datalog.transact(entities);

	return status.success(
		`Mapped Dockerfile to image layers for \`${imageName}\``,
	);
};

function mapSeverity(severity: string): string {
	switch (severity) {
		case "UNKNOWN":
			return "SEVERITY_UNSPECIFIED";
		default:
			return severity;
	}
}
